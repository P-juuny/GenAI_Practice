import sys
from pathlib import Path
import uuid

project_root = Path(__file__).resolve().parent.parent.parent
lang_graph_path = project_root / "src" / "lang_graph"
tools_path = project_root / "src" / "tools"

sys.path.insert(0, str(lang_graph_path))
sys.path.insert(0, str(tools_path))

import gradio as gr
from langgraph.types import Command
from graph import create_graph
from memory import extract_and_save_memory

# ê·¸ë˜í”„ ì „ì—­
graph = create_graph()

# ì„¸ì…˜ë³„ thread_id ì €ì¥
session_threads = {}

def chat(message: str, history: list, request: gr.Request):
    """Gradio ì±„íŒ… í•¨ìˆ˜"""
    
    # ì„¸ì…˜ ID ê°€ì ¸ì˜¤ê¸° (Gradioê°€ ìë™ ìƒì„±)
    session_id = request.session_hash
    
    # ì„¸ì…˜ë³„ thread_id ìƒì„±/ì¡°íšŒ
    if session_id not in session_threads:
        session_threads[session_id] = str(uuid.uuid4())
    
    thread_id = session_threads[session_id]
    config = {"configurable": {"thread_id": thread_id}}
    
    print(f"[SESSION] {session_id} -> thread: {thread_id}")
    
    initial_state = {
        "messages": [{"role": "user", "content": message}],
        "tool_calls": None
    }
    
    final_answer = ""
    
    yield "ğŸ¤” ìƒê° ì¤‘..."
    
    try:
        while True:
            for event in graph.stream(initial_state, config, stream_mode="updates"):
                for node_name, value in event.items():
                    if node_name == "llm":
                        msgs = value.get("messages", [])
                        for m in msgs:
                            if isinstance(m, dict) and m.get("tool_calls"):
                                tool_names = [tc['function']['name'] for tc in m['tool_calls']]
                                yield f"ğŸ”§ ë„êµ¬ ì‚¬ìš© ì¤‘: {', '.join(tool_names)}"
                            if isinstance(m, dict) and m.get("content"):
                                final_answer = m["content"]
                                yield final_answer
                    
                    elif node_name == "tool":
                        yield "âš™ï¸ ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ"
            
            current_state = graph.get_state(config)
            
            if not current_state.next:
                break
            
            if current_state.tasks:
                task = current_state.tasks[0]
                if hasattr(task, 'interrupts') and task.interrupts:
                    print(f"[AUTO APPROVE] {task.interrupts[0].value}")
                    initial_state = Command(resume="y")
                    continue
            
            initial_state = None
        
        if not final_answer:
            final_state = graph.get_state(config)
            final_messages = final_state.values.get("messages", [])
            if final_messages:
                last_msg = final_messages[-1]
                if hasattr(last_msg, "content"):
                    final_answer = last_msg.content
                elif isinstance(last_msg, dict):
                    final_answer = last_msg.get("content", "")
                if final_answer:
                    yield final_answer
        
        if not final_answer:
            yield "âŒ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        else:
            extract_and_save_memory(message, final_answer)
            
    except Exception as e:
        print(f"[ERROR] {e}")
        import traceback
        traceback.print_exc()
        yield f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# Gradio UI
demo = gr.ChatInterface(
    fn=chat,
    title="ğŸ“š ë…¼ë¬¸ ê²€ìƒ‰ AI",
    description="Generative AI ê´€ë ¨ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.",
)

if __name__ == "__main__":
    demo.launch()